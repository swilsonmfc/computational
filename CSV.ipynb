{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Files\n",
    "* Measure performance of reading CSV\n",
    "* Explore Alternatives to Improve Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pandas numpy matplotlib seaborn pyarrow --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a CSV\n",
    "* This short routine will create a file around 3 MB in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100000 rows\n"
     ]
    }
   ],
   "source": [
    "rows=100000\n",
    "print(f'Generating {rows} rows')\n",
    "\n",
    "with open('sample.csv', 'w') as file:   \n",
    "    columns = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "    writer  = csv.DictWriter(file, fieldnames=columns)\n",
    "    writer.writerow(dict(zip(columns, columns)))\n",
    "    \n",
    "    x2 = ['Alpha', 'Beta', 'Gamma', 'Phi', 'Rho']\n",
    "    x3 = ['John', 'Jane', 'William', 'Wilma']\n",
    "    x4 = ['Atlanta', 'New York', 'London', 'Dehli']\n",
    "\n",
    "    for index in range(rows):\n",
    "        writer.writerow(dict([\n",
    "            ('x1', index),\n",
    "            ('x2', random.choice(x2)),\n",
    "            ('x3', random.choice(x3)),\n",
    "            ('x4', random.choice(x4)),\n",
    "            ('x5', str(random.randint(100, 500)))\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5904\n",
      "-rw-r--r--   1 stevew  staff    16835 May  3 10:44 ProducerConsumer.ipynb\n",
      "-rw-r--r--   1 stevew  staff    21297 May  3 10:44 Threads.ipynb\n",
      "-rw-r--r--   1 stevew  staff    29898 May  3 10:44 Multiprocessing.ipynb\n",
      "drwxr-xr-x  12 stevew  staff      408 May  3 10:46 \u001b[34m.git\u001b[m\u001b[m\n",
      "drwx------+ 24 stevew  staff      816 May  5 03:53 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x   6 stevew  staff      204 May  5 12:57 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--   1 stevew  staff     2857 May  5 12:58 CSV.ipynb\n",
      "drwxr-xr-x   9 stevew  staff      306 May  5 12:58 \u001b[34m.\u001b[m\u001b[m\n",
      "-rw-r--r--   1 stevew  staff  2938882 May  5 12:59 sample.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -altr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1 ms ± 72 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('sample.csv', 'r') as file:   \n",
    "    r = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.5 ms ± 1.36 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pd.read_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed\n",
    "* Compression will save on the size of the file\n",
    "* But, you'll pay a penalty reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample.csv')\n",
    "df.to_csv('sample.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9120\n",
      "drwxr-xr-x  12 stevew  staff      408 May  5 13:29 \u001b[34m.\u001b[m\u001b[m\n",
      "drwx------+ 24 stevew  staff      816 May  5 03:53 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x  12 stevew  staff      408 May  3 10:46 \u001b[34m.git\u001b[m\u001b[m\n",
      "drwxr-xr-x   7 stevew  staff      238 May  5 13:12 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--   1 stevew  staff     7813 May  5 13:29 CSV.ipynb\n",
      "-rw-r--r--   1 stevew  staff    29898 May  3 10:44 Multiprocessing.ipynb\n",
      "-rw-r--r--   1 stevew  staff    16835 May  3 10:44 ProducerConsumer.ipynb\n",
      "-rw-r--r--   1 stevew  staff      821 May  5 13:13 SmartOpen.ipynb\n",
      "-rw-r--r--   1 stevew  staff    21297 May  3 10:44 Threads.ipynb\n",
      "-rw-r--r--   1 stevew  staff  2938882 May  5 12:59 sample.csv\n",
      "-rw-r--r--   1 stevew  staff   817656 May  5 13:28 sample.csv.gz\n",
      "-rw-r--r--   1 stevew  staff   818953 May  5 13:16 sample.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 ms ± 824 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pd.read_csv('sample.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet\n",
    "* The engine will attempt to find and use pyarrow (falling back tp fastparquet)\n",
    "* Changing the CSV to parquet will result in a much smaller and faster file access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample.csv')\n",
    "df.to_parquet('sample.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7520\n",
      "-rw-r--r--   1 stevew  staff    16835 May  3 10:44 ProducerConsumer.ipynb\n",
      "-rw-r--r--   1 stevew  staff    21297 May  3 10:44 Threads.ipynb\n",
      "-rw-r--r--   1 stevew  staff    29898 May  3 10:44 Multiprocessing.ipynb\n",
      "drwxr-xr-x  12 stevew  staff      408 May  3 10:46 \u001b[34m.git\u001b[m\u001b[m/\n",
      "drwx------+ 24 stevew  staff      816 May  5 03:53 \u001b[34m..\u001b[m\u001b[m/\n",
      "-rw-r--r--   1 stevew  staff  2938882 May  5 12:59 sample.csv\n",
      "drwxr-xr-x   7 stevew  staff      238 May  5 13:12 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m/\n",
      "-rw-r--r--   1 stevew  staff      821 May  5 13:13 SmartOpen.ipynb\n",
      "-rw-r--r--   1 stevew  staff     5490 May  5 13:15 CSV.ipynb\n",
      "-rw-r--r--   1 stevew  staff   818953 May  5 13:16 sample.parquet\n",
      "drwxr-xr-x  11 stevew  staff      374 May  5 13:16 \u001b[34m.\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls -altr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.7 ms ± 844 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pd.read_parquet('sample.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Pandas\n",
    "* Read in 10000 lines at a time\n",
    "* While there is more overhead in reading from a CPU perspective, we can efficiently iterate over a large file in a memory efficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 ms ± 1.56 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "chunks = pd.read_csv('sample.csv', chunksize=10000)\n",
    "df = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Access\n",
    "* If we wanted to pull one column from a csv performing an operation on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas CSV\n",
    "* All columns must be read in from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index       128\n",
      "x1       800000\n",
      "x2       800000\n",
      "x3       800000\n",
      "x4       800000\n",
      "x5       800000\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['x1', 'x2', 'x3', 'x4', 'x5'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample.csv')\n",
    "print(df.memory_usage())\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.9 ms ± 1.66 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df = pd.read_csv('sample.csv')\n",
    "df.x4 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Parquet\n",
    "* With parquet, we can read in one column (push-down predicate)\n",
    "* Offers an advantage for some use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index       128\n",
      "x4       800000\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['x4'], dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('sample.parquet', columns=['x4'])\n",
    "print(df.memory_usage())\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.6 ms ± 597 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df = pd.read_parquet('sample.parquet', columns=['x4'])\n",
    "df.x4 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
